2025-01-01 16:54:00 [scrapy.utils.log] INFO: Scrapy 2.11.1 started (bot: sggwScraper)
2025-01-01 16:54:00 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 24.2.1 (OpenSSL 3.0.15 3 Sep 2024), cryptography 43.0.0, Platform Windows-10-10.0.19045-SP0
2025-01-01 16:54:00 [scrapy.addons] INFO: Enabled addons:
[]
2025-01-01 16:54:00 [scrapy.extensions.telnet] INFO: Telnet Password: d375b306d7dae1ba
2025-01-01 16:54:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.throttle.AutoThrottle']
2025-01-01 16:54:00 [scrapy.crawler] INFO: Overridden settings:
{'AUTOTHROTTLE_ENABLED': True,
 'AUTOTHROTTLE_START_DELAY': 0.5,
 'AUTOTHROTTLE_TARGET_CONCURRENCY': 4,
 'BOT_NAME': 'sggwScraper',
 'CONCURRENT_REQUESTS': 32,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_errors.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'sggwScraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_HTTP_CODES': [500],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['sggwScraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2025-01-01 16:54:01 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-01-01 16:54:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'sggwScraper.middlewares.SggwscraperDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-01-01 16:54:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-01-01 16:54:02 [scrapy.middleware] INFO: Enabled item pipelines:
['sggwScraper.pipelines.SggwscraperPipeline']
2025-01-01 16:54:02 [scrapy.core.engine] INFO: Spider opened
2025-01-01 16:54:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-01-01 16:54:02 [sggw] INFO: Spider opened: sggw
2025-01-01 16:54:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-01-01 16:54:02 [scrapy-playwright] INFO: Starting download handler
2025-01-01 16:54:02 [scrapy-playwright] INFO: Starting download handler
2025-01-01 16:54:04 [scrapy-playwright] INFO: Launching 3 startup context(s)
2025-01-01 16:54:04 [scrapy-playwright] INFO: Launching browser chromium
2025-01-01 16:54:04 [scrapy-playwright] INFO: Launching 3 startup context(s)
2025-01-01 16:54:04 [scrapy-playwright] INFO: Launching browser chromium
2025-01-01 16:54:04 [scrapy-playwright] INFO: Browser chromium launched
2025-01-01 16:54:04 [scrapy-playwright] INFO: Browser chromium launched
2025-01-01 16:54:04 [scrapy-playwright] INFO: Startup context(s) launched
2025-01-01 16:54:04 [scrapy-playwright] INFO: Startup context(s) launched
2025-01-01 16:55:02 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 7 pages/min), scraped 44 items (at 44 items/min)
2025-01-01 16:55:37 [scrapy.core.engine] INFO: Closing spider (finished)
2025-01-01 16:55:37 [scrapy.extensions.feedexport] INFO: Stored json feed (60 items) in: overview.json
2025-01-01 16:55:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 15687,
 'downloader/request_count': 24,
 'downloader/request_method_count/GET': 24,
 'downloader/response_bytes': 2343892,
 'downloader/response_count': 24,
 'downloader/response_status_count/200': 24,
 'elapsed_time_seconds': 95.006447,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 1, 1, 15, 55, 37, 956975, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 83646,
 'httpcompression/response_count': 2,
 'item_scraped_count': 60,
 'log_count/INFO': 24,
 'playwright/browser_count': 2,
 'playwright/context_count': 7,
 'playwright/context_count/max_concurrent': 4,
 'playwright/context_count/persistent/False': 7,
 'playwright/context_count/remote/False': 7,
 'playwright/page_count': 22,
 'playwright/page_count/max_concurrent': 7,
 'playwright/request_count': 1471,
 'playwright/request_count/aborted': 664,
 'playwright/request_count/method/GET': 1392,
 'playwright/request_count/method/POST': 79,
 'playwright/request_count/navigation': 23,
 'playwright/request_count/resource_type/document': 23,
 'playwright/request_count/resource_type/fetch': 19,
 'playwright/request_count/resource_type/image': 248,
 'playwright/request_count/resource_type/script': 704,
 'playwright/request_count/resource_type/stylesheet': 417,
 'playwright/request_count/resource_type/xhr': 60,
 'playwright/response_count': 792,
 'playwright/response_count/method/GET': 726,
 'playwright/response_count/method/POST': 66,
 'playwright/response_count/resource_type/document': 23,
 'playwright/response_count/resource_type/fetch': 16,
 'playwright/response_count/resource_type/script': 703,
 'playwright/response_count/resource_type/xhr': 50,
 'request_depth_max': 3,
 'response_received_count': 23,
 'scheduler/dequeued': 24,
 'scheduler/dequeued/memory': 24,
 'scheduler/enqueued': 24,
 'scheduler/enqueued/memory': 24,
 'start_time': datetime.datetime(2025, 1, 1, 15, 54, 2, 950528, tzinfo=datetime.timezone.utc)}
2025-01-01 16:55:37 [scrapy.core.engine] INFO: Spider closed (finished)
2025-01-01 16:55:37 [scrapy-playwright] INFO: Closing download handler
2025-01-01 16:55:38 [scrapy-playwright] INFO: Closing browser
2025-01-01 16:55:38 [scrapy-playwright] INFO: Closing download handler
2025-01-01 16:55:39 [scrapy-playwright] INFO: Closing browser
2025-01-01 17:02:31 [scrapy.utils.log] INFO: Scrapy 2.11.1 started (bot: sggwScraper)
2025-01-01 17:02:31 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 24.2.1 (OpenSSL 3.0.15 3 Sep 2024), cryptography 43.0.0, Platform Windows-10-10.0.19045-SP0
2025-01-01 17:02:32 [scrapy.addons] INFO: Enabled addons:
[]
2025-01-01 17:02:32 [scrapy.extensions.telnet] INFO: Telnet Password: 4091955ea1810134
2025-01-01 17:02:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.throttle.AutoThrottle']
2025-01-01 17:02:32 [scrapy.crawler] INFO: Overridden settings:
{'AUTOTHROTTLE_ENABLED': True,
 'AUTOTHROTTLE_START_DELAY': 0.5,
 'AUTOTHROTTLE_TARGET_CONCURRENCY': 4,
 'BOT_NAME': 'sggwScraper',
 'CONCURRENT_REQUESTS': 32,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_errors.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'sggwScraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_HTTP_CODES': [500],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['sggwScraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2025-01-01 17:02:32 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-01-01 17:02:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'sggwScraper.middlewares.SggwscraperDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-01-01 17:02:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-01-01 17:02:34 [scrapy.middleware] INFO: Enabled item pipelines:
['sggwScraper.pipelines.SggwscraperPipeline']
2025-01-01 17:02:34 [scrapy.core.engine] INFO: Spider opened
2025-01-01 17:02:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-01-01 17:02:34 [sggw] INFO: Spider opened: sggw
2025-01-01 17:02:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-01-01 17:02:34 [scrapy-playwright] INFO: Starting download handler
2025-01-01 17:02:34 [scrapy-playwright] INFO: Starting download handler
2025-01-01 17:02:35 [scrapy-playwright] INFO: Launching 3 startup context(s)
2025-01-01 17:02:35 [scrapy-playwright] INFO: Launching browser chromium
2025-01-01 17:02:35 [scrapy-playwright] INFO: Launching 3 startup context(s)
2025-01-01 17:02:35 [scrapy-playwright] INFO: Launching browser chromium
2025-01-01 17:02:36 [scrapy-playwright] INFO: Browser chromium launched
2025-01-01 17:02:36 [scrapy-playwright] INFO: Startup context(s) launched
2025-01-01 17:02:36 [scrapy-playwright] INFO: Browser chromium launched
2025-01-01 17:02:36 [scrapy-playwright] INFO: Startup context(s) launched
2025-01-01 17:03:34 [scrapy.extensions.logstats] INFO: Crawled 11 pages (at 11 pages/min), scraped 48 items (at 48 items/min)
2025-01-01 17:03:41 [scrapy-playwright] INFO: Launching browser chromium
2025-01-01 17:03:41 [scrapy-playwright] INFO: Launching browser chromium
2025-01-01 17:03:41 [scrapy-playwright] INFO: Launching browser chromium
2025-01-01 17:03:41 [scrapy-playwright] INFO: Launching browser chromium
2025-01-01 17:03:41 [scrapy-playwright] INFO: Launching browser chromium
2025-01-01 17:03:44 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-01-01 17:03:44 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-01-01 17:03:44 [scrapy-playwright] INFO: Launching browser chromium
2025-01-01 17:03:44 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/author/WULS908bcd26d5b041b4a6eb70ff94428ae0?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BOlena%2BKulykovets%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 397, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 296, in _create_page
    ctx_wrapper = await self._create_browser_context(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 257, in _create_browser_context
    await self._maybe_launch_browser()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 205, in _maybe_launch_browser
    self.browser = await self.browser_type.launch(**self.config.launch_options)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 14398, in launch
    await self._impl_obj.launch(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_browser_type.py", line 95, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserType.launch: Connection closed while reading from the driver
2025-01-01 17:03:44 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/author/WULS15172cc1b4744ad0a7e52b8c03b7a3cf?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BZdzis%25C5%2582aw%2BGajewski%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 397, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 296, in _create_page
    ctx_wrapper = await self._create_browser_context(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 257, in _create_browser_context
    await self._maybe_launch_browser()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 205, in _maybe_launch_browser
    self.browser = await self.browser_type.launch(**self.config.launch_options)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 14398, in launch
    await self._impl_obj.launch(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_browser_type.py", line 95, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserType.launch: Connection closed while reading from the driver
2025-01-01 17:03:44 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/author/WULSe55da96565fc41a6b364693e2c34e30d?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BMonika%2BG%25C4%2599bska%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 397, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 296, in _create_page
    ctx_wrapper = await self._create_browser_context(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 257, in _create_browser_context
    await self._maybe_launch_browser()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 205, in _maybe_launch_browser
    self.browser = await self.browser_type.launch(**self.config.launch_options)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 14398, in launch
    await self._impl_obj.launch(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_browser_type.py", line 95, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserType.launch: Connection closed while reading from the driver
2025-01-01 17:03:44 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/author/WULS5dd3c0e2e3954d188b2f9b82916742c5?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BAgnieszka%2BTul-Krzyszczuk%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 397, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 296, in _create_page
    ctx_wrapper = await self._create_browser_context(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 257, in _create_browser_context
    await self._maybe_launch_browser()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 205, in _maybe_launch_browser
    self.browser = await self.browser_type.launch(**self.config.launch_options)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 14398, in launch
    await self._impl_obj.launch(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_browser_type.py", line 95, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserType.launch: Connection closed while reading from the driver
2025-01-01 17:03:44 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/author/WULS9d733cda0b0a4c36a59f7e8b15d26c36?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BAgnieszka%2BWerenowska%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 397, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 296, in _create_page
    ctx_wrapper = await self._create_browser_context(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 257, in _create_browser_context
    await self._maybe_launch_browser()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 205, in _maybe_launch_browser
    self.browser = await self.browser_type.launch(**self.config.launch_options)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 14398, in launch
    await self._impl_obj.launch(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_browser_type.py", line 95, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserType.launch: Connection closed while reading from the driver
2025-01-01 17:03:44 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/author/WULS0570793a59334a14b73b1d51712de33b?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BDaria%2BWojewodzic%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 397, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 296, in _create_page
    ctx_wrapper = await self._create_browser_context(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 257, in _create_browser_context
    await self._maybe_launch_browser()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 205, in _maybe_launch_browser
    self.browser = await self.browser_type.launch(**self.config.launch_options)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 14398, in launch
    await self._impl_obj.launch(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_browser_type.py", line 95, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserType.launch: Connection closed while reading from the driver
2025-01-01 17:03:48 [scrapy-playwright] INFO: Launching browser chromium
2025-01-01 17:03:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://bw.sggw.edu.pl/info/author/WULS21208ba7a4bd49c1accf1c4e56be6f7d?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BRomuald%2BZabielski%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse> (referer: https://bw.sggw.edu.pl/globalResultList.seam?q=&oa=false&r=author&tab=PEOPLE&conversationPropagation=begin&lang=en&qp=openAccess%3Dfalse&p=xyz&pn=1)
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\defer.py", line 295, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\Desktop\PYTHON\WebScraping\SGGW_scraper\Scraper\sggwScraper\sggwScraper\spiders\sggw.py", line 198, in parse_scientist
    await page.close()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 9767, in close
    await self._impl_obj.close(runBeforeUnload=run_before_unload, reason=reason)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_page.py", line 809, in close
    raise e
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_page.py", line 804, in close
    await self._channel.send("close", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.close: Connection closed while reading from the driver
2025-01-01 17:03:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://bw.sggw.edu.pl/info/author/WULSb23720da404944e28665261e73b03d5f?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BWojciech%2BPiz%25C5%2582o%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse> (referer: https://bw.sggw.edu.pl/globalResultList.seam?q=&oa=false&r=author&tab=PEOPLE&conversationPropagation=begin&lang=en&qp=openAccess%3Dfalse&p=xyz&pn=1)
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\defer.py", line 295, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\Desktop\PYTHON\WebScraping\SGGW_scraper\Scraper\sggwScraper\sggwScraper\spiders\sggw.py", line 198, in parse_scientist
    await page.close()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 9767, in close
    await self._impl_obj.close(runBeforeUnload=run_before_unload, reason=reason)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_page.py", line 809, in close
    raise e
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_page.py", line 804, in close
    await self._channel.send("close", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.close: Connection closed while reading from the driver
2025-01-01 17:03:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://bw.sggw.edu.pl/info/author/WULSf2bfbb8df4ad4c6ea90b0068c8317618?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BIrena%2BOzimek%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse> (referer: https://bw.sggw.edu.pl/globalResultList.seam?q=&oa=false&r=author&tab=PEOPLE&conversationPropagation=begin&lang=en&qp=openAccess%3Dfalse&p=xyz&pn=1)
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\defer.py", line 295, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\Desktop\PYTHON\WebScraping\SGGW_scraper\Scraper\sggwScraper\sggwScraper\spiders\sggw.py", line 198, in parse_scientist
    await page.close()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 9767, in close
    await self._impl_obj.close(runBeforeUnload=run_before_unload, reason=reason)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_page.py", line 809, in close
    raise e
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_page.py", line 804, in close
    await self._channel.send("close", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.close: Connection closed while reading from the driver
2025-01-01 17:03:49 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/author/WULS074090d8ba344c568bc1c8f2fba917f9?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BBart%25C5%2582omiej%2BWysocza%25C5%2584ski%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 397, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 296, in _create_page
    ctx_wrapper = await self._create_browser_context(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 257, in _create_browser_context
    await self._maybe_launch_browser()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 205, in _maybe_launch_browser
    self.browser = await self.browser_type.launch(**self.config.launch_options)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 14398, in launch
    await self._impl_obj.launch(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_browser_type.py", line 95, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserType.launch: Connection closed while reading from the driver
2025-01-01 17:03:53 [scrapy-playwright] INFO: Launching browser chromium
2025-01-01 17:03:54 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/author/WULS0ab19354788e4af08d9addf2eefbb33a?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BAbhishek%2BRath%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 397, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 296, in _create_page
    ctx_wrapper = await self._create_browser_context(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 257, in _create_browser_context
    await self._maybe_launch_browser()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 205, in _maybe_launch_browser
    self.browser = await self.browser_type.launch(**self.config.launch_options)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 14398, in launch
    await self._impl_obj.launch(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_browser_type.py", line 95, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserType.launch: Connection closed while reading from the driver
2025-01-01 17:03:56 [scrapy-playwright] INFO: Launching browser chromium
2025-01-01 17:03:59 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/author/WULS0ad1929e54db456d82390517e0888ba7?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BMonika%2BSo%25C5%2582oniewicz%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 397, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 296, in _create_page
    ctx_wrapper = await self._create_browser_context(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 257, in _create_browser_context
    await self._maybe_launch_browser()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 205, in _maybe_launch_browser
    self.browser = await self.browser_type.launch(**self.config.launch_options)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 14398, in launch
    await self._impl_obj.launch(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_browser_type.py", line 95, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserType.launch: Connection closed while reading from the driver
2025-01-01 17:03:59 [scrapy.extensions.feedexport] INFO: Stored json feed (51 items) in: overview.json
2025-01-01 17:03:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 9,
 'downloader/exception_type_count/builtins.Exception': 9,
 'downloader/request_bytes': 15687,
 'downloader/request_count': 24,
 'downloader/request_method_count/GET': 24,
 'downloader/response_bytes': 1564689,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 85.230199,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2025, 1, 1, 16, 3, 59, 692304, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 83646,
 'httpcompression/response_count': 2,
 'item_scraped_count': 51,
 'log_count/ERROR': 12,
 'log_count/INFO': 34,
 'playwright/browser_count': 2,
 'playwright/context_count': 7,
 'playwright/context_count/max_concurrent': 4,
 'playwright/context_count/persistent/False': 7,
 'playwright/context_count/remote/False': 7,
 'playwright/page_count': 18,
 'playwright/page_count/max_concurrent': 8,
 'playwright/request_count': 1198,
 'playwright/request_count/aborted': 552,
 'playwright/request_count/method/GET': 1146,
 'playwright/request_count/method/POST': 52,
 'playwright/request_count/navigation': 19,
 'playwright/request_count/resource_type/document': 19,
 'playwright/request_count/resource_type/fetch': 9,
 'playwright/request_count/resource_type/image': 207,
 'playwright/request_count/resource_type/script': 575,
 'playwright/request_count/resource_type/stylesheet': 345,
 'playwright/request_count/resource_type/xhr': 43,
 'playwright/response_count': 528,
 'playwright/response_count/method/GET': 483,
 'playwright/response_count/method/POST': 45,
 'playwright/response_count/resource_type/document': 19,
 'playwright/response_count/resource_type/fetch': 7,
 'playwright/response_count/resource_type/script': 464,
 'playwright/response_count/resource_type/xhr': 38,
 'request_depth_max': 3,
 'response_received_count': 14,
 'scheduler/dequeued': 24,
 'scheduler/dequeued/memory': 24,
 'scheduler/enqueued': 24,
 'scheduler/enqueued/memory': 24,
 'spider_exceptions/Exception': 3,
 'start_time': datetime.datetime(2025, 1, 1, 16, 2, 34, 462105, tzinfo=datetime.timezone.utc)}
2025-01-01 17:03:59 [scrapy.core.engine] INFO: Spider closed (shutdown)
2025-01-01 17:03:59 [scrapy-playwright] INFO: Closing download handler
2025-01-01 17:03:59 [scrapy-playwright] INFO: Closing download handler
2025-01-01 17:05:10 [scrapy.utils.log] INFO: Scrapy 2.11.1 started (bot: sggwScraper)
2025-01-01 17:05:11 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 24.2.1 (OpenSSL 3.0.15 3 Sep 2024), cryptography 43.0.0, Platform Windows-10-10.0.19045-SP0
2025-01-01 17:05:11 [scrapy.addons] INFO: Enabled addons:
[]
2025-01-01 17:05:11 [scrapy.extensions.telnet] INFO: Telnet Password: bb3e931c1580cec9
2025-01-01 17:05:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.throttle.AutoThrottle']
2025-01-01 17:05:11 [scrapy.crawler] INFO: Overridden settings:
{'AUTOTHROTTLE_ENABLED': True,
 'AUTOTHROTTLE_START_DELAY': 0.5,
 'AUTOTHROTTLE_TARGET_CONCURRENCY': 4,
 'BOT_NAME': 'sggwScraper',
 'CONCURRENT_REQUESTS': 32,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_errors.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'sggwScraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_HTTP_CODES': [500],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['sggwScraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2025-01-01 17:05:11 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-01-01 17:05:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'sggwScraper.middlewares.SggwscraperDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-01-01 17:05:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-01-01 17:05:13 [scrapy.middleware] INFO: Enabled item pipelines:
['sggwScraper.pipelines.SggwscraperPipeline']
2025-01-01 17:05:13 [scrapy.core.engine] INFO: Spider opened
2025-01-01 17:05:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-01-01 17:05:13 [sggw] INFO: Spider opened: sggw
2025-01-01 17:05:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-01-01 17:05:13 [scrapy-playwright] INFO: Starting download handler
2025-01-01 17:05:13 [scrapy-playwright] INFO: Starting download handler
2025-01-01 17:05:14 [scrapy-playwright] INFO: Launching 3 startup context(s)
2025-01-01 17:05:14 [scrapy-playwright] INFO: Launching browser chromium
2025-01-01 17:05:14 [scrapy-playwright] INFO: Launching 3 startup context(s)
2025-01-01 17:05:14 [scrapy-playwright] INFO: Launching browser chromium
2025-01-01 17:05:14 [scrapy-playwright] INFO: Browser chromium launched
2025-01-01 17:05:14 [scrapy-playwright] INFO: Browser chromium launched
2025-01-01 17:05:14 [scrapy-playwright] INFO: Startup context(s) launched
2025-01-01 17:05:14 [scrapy-playwright] INFO: Startup context(s) launched
2025-01-01 17:06:00 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-01-01 17:06:00 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-01-01 17:06:02 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-01-01 17:08:19 [scrapy.utils.log] INFO: Scrapy 2.11.1 started (bot: sggwScraper)
2025-01-01 17:08:19 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 24.2.1 (OpenSSL 3.0.15 3 Sep 2024), cryptography 43.0.0, Platform Windows-10-10.0.19045-SP0
2025-01-01 17:08:19 [scrapy.addons] INFO: Enabled addons:
[]
2025-01-01 17:08:20 [scrapy.extensions.telnet] INFO: Telnet Password: 22bdb9937e42eb4b
2025-01-01 17:08:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.throttle.AutoThrottle']
2025-01-01 17:08:20 [scrapy.crawler] INFO: Overridden settings:
{'AUTOTHROTTLE_ENABLED': True,
 'AUTOTHROTTLE_START_DELAY': 0.5,
 'AUTOTHROTTLE_TARGET_CONCURRENCY': 4,
 'BOT_NAME': 'sggwScraper',
 'CONCURRENT_REQUESTS': 32,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_errors.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'sggwScraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_HTTP_CODES': [500],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['sggwScraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2025-01-01 17:08:20 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-01-01 17:08:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'sggwScraper.middlewares.SggwscraperDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-01-01 17:08:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-01-01 17:08:22 [scrapy.middleware] INFO: Enabled item pipelines:
['sggwScraper.pipelines.SggwscraperPipeline']
2025-01-01 17:08:22 [scrapy.core.engine] INFO: Spider opened
2025-01-01 17:08:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-01-01 17:08:22 [sggw] INFO: Spider opened: sggw
2025-01-01 17:08:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-01-01 17:08:22 [scrapy-playwright] INFO: Starting download handler
2025-01-01 17:08:22 [scrapy-playwright] INFO: Starting download handler
2025-01-01 17:08:23 [scrapy-playwright] INFO: Launching 3 startup context(s)
2025-01-01 17:08:23 [scrapy-playwright] INFO: Launching browser chromium
2025-01-01 17:08:23 [scrapy-playwright] INFO: Launching 3 startup context(s)
2025-01-01 17:08:23 [scrapy-playwright] INFO: Launching browser chromium
2025-01-01 17:08:23 [scrapy-playwright] INFO: Browser chromium launched
2025-01-01 17:08:23 [scrapy-playwright] INFO: Browser chromium launched
2025-01-01 17:08:23 [scrapy-playwright] INFO: Startup context(s) launched
2025-01-01 17:08:23 [scrapy-playwright] INFO: Startup context(s) launched
2025-01-01 17:09:22 [scrapy.extensions.logstats] INFO: Crawled 11 pages (at 11 pages/min), scraped 48 items (at 48 items/min)
2025-01-01 17:09:42 [scrapy.core.engine] INFO: Closing spider (finished)
2025-01-01 17:09:42 [scrapy.extensions.feedexport] INFO: Stored json feed (60 items) in: overview.json
2025-01-01 17:09:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 15687,
 'downloader/request_count': 24,
 'downloader/request_method_count/GET': 24,
 'downloader/response_bytes': 2343615,
 'downloader/response_count': 24,
 'downloader/response_status_count/200': 24,
 'elapsed_time_seconds': 80.236157,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 1, 1, 16, 9, 42, 457745, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 83646,
 'httpcompression/response_count': 2,
 'item_scraped_count': 60,
 'log_count/INFO': 24,
 'playwright/browser_count': 2,
 'playwright/context_count': 7,
 'playwright/context_count/max_concurrent': 4,
 'playwright/context_count/persistent/False': 7,
 'playwright/context_count/remote/False': 7,
 'playwright/page_count': 22,
 'playwright/page_count/max_concurrent': 6,
 'playwright/request_count': 1461,
 'playwright/request_count/aborted': 663,
 'playwright/request_count/method/GET': 1392,
 'playwright/request_count/method/POST': 69,
 'playwright/request_count/navigation': 23,
 'playwright/request_count/resource_type/document': 23,
 'playwright/request_count/resource_type/fetch': 11,
 'playwright/request_count/resource_type/image': 248,
 'playwright/request_count/resource_type/script': 704,
 'playwright/request_count/resource_type/stylesheet': 417,
 'playwright/request_count/resource_type/xhr': 58,
 'playwright/response_count': 782,
 'playwright/response_count/method/GET': 725,
 'playwright/response_count/method/POST': 57,
 'playwright/response_count/resource_type/document': 23,
 'playwright/response_count/resource_type/fetch': 8,
 'playwright/response_count/resource_type/script': 702,
 'playwright/response_count/resource_type/xhr': 49,
 'request_depth_max': 3,
 'response_received_count': 23,
 'scheduler/dequeued': 24,
 'scheduler/dequeued/memory': 24,
 'scheduler/enqueued': 24,
 'scheduler/enqueued/memory': 24,
 'start_time': datetime.datetime(2025, 1, 1, 16, 8, 22, 221588, tzinfo=datetime.timezone.utc)}
2025-01-01 17:09:42 [scrapy.core.engine] INFO: Spider closed (finished)
2025-01-01 17:09:42 [scrapy-playwright] INFO: Closing download handler
2025-01-01 17:09:42 [scrapy-playwright] INFO: Closing browser
2025-01-01 17:09:43 [scrapy-playwright] INFO: Closing download handler
2025-01-01 17:09:43 [scrapy-playwright] INFO: Closing browser
2025-01-01 17:10:28 [scrapy.utils.log] INFO: Scrapy 2.11.1 started (bot: sggwScraper)
2025-01-01 17:10:28 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 24.2.1 (OpenSSL 3.0.15 3 Sep 2024), cryptography 43.0.0, Platform Windows-10-10.0.19045-SP0
2025-01-01 17:10:28 [scrapy.addons] INFO: Enabled addons:
[]
2025-01-01 17:10:28 [scrapy.extensions.telnet] INFO: Telnet Password: a95f3e839e8df227
2025-01-01 17:10:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.throttle.AutoThrottle']
2025-01-01 17:10:28 [scrapy.crawler] INFO: Overridden settings:
{'AUTOTHROTTLE_ENABLED': True,
 'AUTOTHROTTLE_START_DELAY': 0.5,
 'AUTOTHROTTLE_TARGET_CONCURRENCY': 4,
 'BOT_NAME': 'sggwScraper',
 'CONCURRENT_REQUESTS': 32,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_errors.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'sggwScraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_HTTP_CODES': [500],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['sggwScraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2025-01-01 17:10:28 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-01-01 17:10:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'sggwScraper.middlewares.SggwscraperDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-01-01 17:10:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-01-01 17:10:30 [scrapy.middleware] INFO: Enabled item pipelines:
['sggwScraper.pipelines.SggwscraperPipeline']
2025-01-01 17:10:30 [scrapy.core.engine] INFO: Spider opened
2025-01-01 17:10:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-01-01 17:10:30 [sggw] INFO: Spider opened: sggw
2025-01-01 17:10:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-01-01 17:10:30 [scrapy-playwright] INFO: Starting download handler
2025-01-01 17:10:30 [scrapy-playwright] INFO: Starting download handler
2025-01-01 17:10:31 [scrapy-playwright] INFO: Launching 3 startup context(s)
2025-01-01 17:10:31 [scrapy-playwright] INFO: Launching browser chromium
2025-01-01 17:10:31 [scrapy-playwright] INFO: Launching 3 startup context(s)
2025-01-01 17:10:31 [scrapy-playwright] INFO: Launching browser chromium
2025-01-01 17:10:31 [scrapy-playwright] INFO: Browser chromium launched
2025-01-01 17:10:31 [scrapy-playwright] INFO: Browser chromium launched
2025-01-01 17:10:31 [scrapy-playwright] INFO: Startup context(s) launched
2025-01-01 17:10:31 [scrapy-playwright] INFO: Startup context(s) launched
2025-01-01 17:11:10 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS81441c04c672447d86312eb6f665d324?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BHanna%2BG%25C3%25B3rska-Warsewicz%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:15 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS4d84c4f309fa474e959ae6c863258045?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BTomasz%2BRokicki%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:18 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS2cdf660402704268a5f7e5653239927c?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BMarcin%2BRatajczak%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:19 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS17de172746cc4655a0bec7eb7f8f15bf?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BEwa%2BStawicka%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:21 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULSd3527b40fc054f47af198345d84f4d7d?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BPiotr%2BPietrzak%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:23 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULSf6dcbeae95744d2da7a9e3712f1e080b?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BJoanna%2BPaliszkiewicz%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:26 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS30126978a056423e800da95795a5feb3?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BKatarzyna%2B%25C5%2581ukasiewicz%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:29 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULSf2bfbb8df4ad4c6ea90b0068c8317618?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BIrena%2BOzimek%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:30 [scrapy.extensions.logstats] INFO: Crawled 11 pages (at 11 pages/min), scraped 40 items (at 40 items/min)
2025-01-01 17:11:33 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS876bf269000646d2b25dd38f651d14c4?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BHubert%2BSzczepaniuk%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:33 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS21208ba7a4bd49c1accf1c4e56be6f7d?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BRomuald%2BZabielski%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:33 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULSb23720da404944e28665261e73b03d5f?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BWojciech%2BPiz%25C5%2582o%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:37 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULSe55da96565fc41a6b364693e2c34e30d?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BMonika%2BG%25C4%2599bska%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:40 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS5dd3c0e2e3954d188b2f9b82916742c5?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BAgnieszka%2BTul-Krzyszczuk%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:40 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS15172cc1b4744ad0a7e52b8c03b7a3cf?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BZdzis%25C5%2582aw%2BGajewski%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:47 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS9d733cda0b0a4c36a59f7e8b15d26c36?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BAgnieszka%2BWerenowska%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:47 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS0570793a59334a14b73b1d51712de33b?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BDaria%2BWojewodzic%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:47 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS908bcd26d5b041b4a6eb70ff94428ae0?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BOlena%2BKulykovets%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:50 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS074090d8ba344c568bc1c8f2fba917f9?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BBart%25C5%2582omiej%2BWysocza%25C5%2584ski%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:50 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS0ab19354788e4af08d9addf2eefbb33a?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BAbhishek%2BRath%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:50 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS0ad1929e54db456d82390517e0888ba7?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BMonika%2BSo%25C5%2582oniewicz%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:53 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS81441c04c672447d86312eb6f665d324?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BHanna%2BG%25C3%25B3rska-Warsewicz%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:55 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS4d84c4f309fa474e959ae6c863258045?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BTomasz%2BRokicki%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:11:58 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS2cdf660402704268a5f7e5653239927c?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BMarcin%2BRatajczak%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:00 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS17de172746cc4655a0bec7eb7f8f15bf?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BEwa%2BStawicka%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:02 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULSd3527b40fc054f47af198345d84f4d7d?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BPiotr%2BPietrzak%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:09 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS30126978a056423e800da95795a5feb3?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BKatarzyna%2B%25C5%2581ukasiewicz%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:09 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULSf6dcbeae95744d2da7a9e3712f1e080b?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BJoanna%2BPaliszkiewicz%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:09 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULSf2bfbb8df4ad4c6ea90b0068c8317618?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BIrena%2BOzimek%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:15 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULSb23720da404944e28665261e73b03d5f?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BWojciech%2BPiz%25C5%2582o%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:15 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS21208ba7a4bd49c1accf1c4e56be6f7d?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BRomuald%2BZabielski%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:20 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS15172cc1b4744ad0a7e52b8c03b7a3cf?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BZdzis%25C5%2582aw%2BGajewski%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:20 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS876bf269000646d2b25dd38f651d14c4?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BHubert%2BSzczepaniuk%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:25 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS5dd3c0e2e3954d188b2f9b82916742c5?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BAgnieszka%2BTul-Krzyszczuk%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:25 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS0570793a59334a14b73b1d51712de33b?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BDaria%2BWojewodzic%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:25 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS908bcd26d5b041b4a6eb70ff94428ae0?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BOlena%2BKulykovets%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:26 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULSe55da96565fc41a6b364693e2c34e30d?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BMonika%2BG%25C4%2599bska%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:29 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS9d733cda0b0a4c36a59f7e8b15d26c36?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BAgnieszka%2BWerenowska%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:29 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS0ad1929e54db456d82390517e0888ba7?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BMonika%2BSo%25C5%2582oniewicz%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:30 [scrapy.extensions.logstats] INFO: Crawled 42 pages (at 31 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:12:30 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS0ab19354788e4af08d9addf2eefbb33a?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BAbhishek%2BRath%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:35 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS074090d8ba344c568bc1c8f2fba917f9?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BBart%25C5%2582omiej%2BWysocza%25C5%2584ski%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:35 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS81441c04c672447d86312eb6f665d324?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BHanna%2BG%25C3%25B3rska-Warsewicz%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:39 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS4d84c4f309fa474e959ae6c863258045?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BTomasz%2BRokicki%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:40 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS2cdf660402704268a5f7e5653239927c?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BMarcin%2BRatajczak%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:42 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULS17de172746cc4655a0bec7eb7f8f15bf?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BEwa%2BStawicka%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:48 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULSf2bfbb8df4ad4c6ea90b0068c8317618?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BIrena%2BOzimek%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:12:48 [sggw] ERROR: Error in parse_scientist, 'NoneType' object has no attribute 'strip' https://bw.sggw.edu.pl/info/author/WULSd3527b40fc054f47af198345d84f4d7d?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BPiotr%2BPietrzak%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse
2025-01-01 17:13:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 7 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:14:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:15:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:15:50 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/author/WULS21208ba7a4bd49c1accf1c4e56be6f7d?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BRomuald%2BZabielski%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 431, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 460, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 560, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 8969, in goto
    await self._impl_obj.goto(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_page.py", line 553, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_frame.py", line 145, in goto
    await self._channel.send("goto", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 180000ms exceeded.
Call log:
navigating to "https://bw.sggw.edu.pl/info/author/WULS21208ba7a4bd49c1accf1c4e56be6f7d?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BRomuald%2BZabielski%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse", waiting until "load"

2025-01-01 17:15:50 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/author/WULSb23720da404944e28665261e73b03d5f?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BWojciech%2BPiz%25C5%2582o%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 431, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 460, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 560, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 8969, in goto
    await self._impl_obj.goto(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_page.py", line 553, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_frame.py", line 145, in goto
    await self._channel.send("goto", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.goto: Timeout 180000ms exceeded.
Call log:
navigating to "https://bw.sggw.edu.pl/info/author/WULSb23720da404944e28665261e73b03d5f?r=author&tab=&title=Person%2Bprofile%2B%25E2%2580%2593%2BWojciech%2BPiz%25C5%2582o%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en&qp=openAccess%3Dfalse", waiting until "load"

2025-01-01 17:16:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:17:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:18:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:19:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:20:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:21:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:22:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:23:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:24:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:25:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:26:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:27:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:28:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:29:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:30:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:31:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:32:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:33:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:34:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:35:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:36:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:37:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:38:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:39:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:40:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:41:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:42:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:43:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:44:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:45:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:46:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:47:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:48:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:49:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:50:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:51:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:52:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:53:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:54:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:55:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:56:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:57:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:58:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 17:59:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:00:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:01:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:02:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:03:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:04:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:05:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:06:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:07:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:08:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:09:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:10:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:11:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:12:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:13:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:14:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:15:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:16:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:17:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:18:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:19:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:20:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:21:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:22:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:23:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:24:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:25:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:26:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:27:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:28:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:29:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:30:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:31:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:32:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:33:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:34:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:35:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:36:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:37:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:38:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:39:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:40:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:41:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:42:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:43:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:44:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:45:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:46:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:47:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:48:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:49:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:50:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:51:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:52:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:53:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:54:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:55:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:56:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:57:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:58:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 18:59:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:00:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:01:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:02:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:03:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:04:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:05:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:06:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:07:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:08:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:09:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:10:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:11:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:12:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:13:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:14:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:15:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:16:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:17:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:18:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:19:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:20:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:21:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:22:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:23:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:24:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:25:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:26:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:27:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:28:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:29:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:30:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:31:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:32:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:33:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:34:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:35:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:36:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:37:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:38:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:39:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:40:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:41:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:42:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:43:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:44:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:45:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:46:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:47:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:48:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:49:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:50:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:51:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:52:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:53:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:54:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:55:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:56:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:57:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:58:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 19:59:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:00:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:01:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:02:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:03:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:04:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:05:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:06:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:07:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:08:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:09:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:10:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:11:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:12:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:13:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:14:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:15:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:16:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:17:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:18:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:19:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:20:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:21:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:22:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:23:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:24:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:25:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:26:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:27:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:28:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:29:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:30:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:31:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:32:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:33:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:34:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:35:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:36:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:37:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:38:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:39:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:40:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:41:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:42:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:43:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:44:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:45:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:46:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:47:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:48:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:49:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:50:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:51:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:52:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:53:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:54:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:55:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:56:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:57:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:58:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 20:59:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:00:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:01:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:02:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:03:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:04:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:05:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:06:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:07:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:08:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:09:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:10:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:11:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:12:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:13:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:14:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:15:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:16:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:17:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:18:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:19:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:20:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:21:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:22:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:23:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:24:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:25:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:26:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:27:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:28:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:29:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:30:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:31:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:32:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:33:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:34:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:35:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:36:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:37:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:38:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:39:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:40:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:41:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:42:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:43:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:44:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:45:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:46:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:47:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:48:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:49:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:50:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:51:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:52:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:53:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:54:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:55:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:56:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:57:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:58:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 21:59:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:00:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:01:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:02:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:03:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:04:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:05:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:06:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:07:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:08:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:09:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:10:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:11:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:12:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:13:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:14:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:15:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:16:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:17:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:18:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:19:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:20:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:21:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:22:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:23:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:24:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:25:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:26:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:27:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:28:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:29:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:30:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:31:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:32:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:33:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:34:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:35:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:36:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:37:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:38:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:39:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:40:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:41:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:42:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:43:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:44:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:45:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:46:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:47:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:48:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:49:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:50:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:51:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:52:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:53:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:54:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:55:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:56:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:57:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:58:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 22:59:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:00:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:01:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:02:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:03:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:04:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:05:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:06:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:07:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:08:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:09:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:10:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:11:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:12:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:13:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:14:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:15:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:16:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:17:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:18:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:19:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:20:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:21:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:22:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:23:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:24:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:25:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:26:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:27:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:28:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:29:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:30:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:31:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:32:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:33:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:34:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:35:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:36:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:37:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:38:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:39:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:40:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:41:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:42:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:43:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:44:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:45:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:46:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
2025-01-01 23:47:30 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 0 pages/min), scraped 40 items (at 0 items/min)
