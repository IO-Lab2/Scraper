2024-12-15 15:36:55 [scrapy.utils.log] INFO: Scrapy 2.11.1 started (bot: sggwScraper)
2024-12-15 15:36:56 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 24.2.1 (OpenSSL 3.0.15 3 Sep 2024), cryptography 43.0.0, Platform Windows-10-10.0.19045-SP0
2024-12-15 15:36:56 [scrapy.addons] INFO: Enabled addons:
[]
2024-12-15 15:36:56 [scrapy.extensions.telnet] INFO: Telnet Password: 63ec60865e966765
2024-12-15 15:36:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.throttle.AutoThrottle']
2024-12-15 15:36:56 [scrapy.crawler] INFO: Overridden settings:
{'AUTOTHROTTLE_ENABLED': True,
 'AUTOTHROTTLE_MAX_DELAY': 30,
 'AUTOTHROTTLE_START_DELAY': 0.2,
 'AUTOTHROTTLE_TARGET_CONCURRENCY': 4,
 'BOT_NAME': 'sggwScraper',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_errors.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'sggwScraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_HTTP_CODES': [500],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['sggwScraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2024-12-15 15:36:56 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2024-12-15 15:36:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'sggwScraper.middlewares.SggwscraperDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2024-12-15 15:36:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2024-12-15 15:36:58 [scrapy.middleware] INFO: Enabled item pipelines:
['sggwScraper.pipelines.SggwscraperPipeline']
2024-12-15 15:36:58 [scrapy.core.engine] INFO: Spider opened
2024-12-15 15:36:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2024-12-15 15:36:58 [publications] INFO: Spider opened: publications
2024-12-15 15:36:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2024-12-15 15:36:58 [scrapy-playwright] INFO: Starting download handler
2024-12-15 15:36:58 [scrapy-playwright] INFO: Starting download handler
2024-12-15 15:36:59 [scrapy-playwright] INFO: Launching 3 startup context(s)
2024-12-15 15:36:59 [scrapy-playwright] INFO: Launching browser chromium
2024-12-15 15:36:59 [scrapy-playwright] INFO: Launching 3 startup context(s)
2024-12-15 15:36:59 [scrapy-playwright] INFO: Launching browser chromium
2024-12-15 15:37:00 [scrapy-playwright] INFO: Browser chromium launched
2024-12-15 15:37:00 [scrapy-playwright] INFO: Browser chromium launched
2024-12-15 15:37:00 [scrapy-playwright] INFO: Startup context(s) launched
2024-12-15 15:37:00 [scrapy-playwright] INFO: Startup context(s) launched
2024-12-15 15:37:48 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2024-12-15 15:37:48 [scrapy.core.engine] INFO: Closing spider (shutdown)
2024-12-15 15:37:48 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=bst&pn=1>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 431, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 460, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 560, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 8969, in goto
    await self._impl_obj.goto(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_page.py", line 553, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_frame.py", line 145, in goto
    await self._channel.send("goto", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.goto: Connection closed while reading from the driver
2024-12-15 15:37:48 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=bst&pn=2>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 431, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 460, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 560, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 8969, in goto
    await self._impl_obj.goto(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_page.py", line 553, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_frame.py", line 145, in goto
    await self._channel.send("goto", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.goto: Connection closed while reading from the driver
2024-12-15 15:37:48 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=bst&pn=8>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 431, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 460, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 560, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 8969, in goto
    await self._impl_obj.goto(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_page.py", line 553, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_frame.py", line 145, in goto
    await self._channel.send("goto", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.goto: Connection closed while reading from the driver
2024-12-15 15:37:48 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=bst&pn=7>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 431, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 460, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 560, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 8969, in goto
    await self._impl_obj.goto(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_page.py", line 553, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_frame.py", line 145, in goto
    await self._channel.send("goto", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.goto: Connection closed while reading from the driver
2024-12-15 15:37:48 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=bst&pn=5>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 431, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 460, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 560, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 8969, in goto
    await self._impl_obj.goto(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_page.py", line 553, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_frame.py", line 145, in goto
    await self._channel.send("goto", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.goto: Connection closed while reading from the driver
2024-12-15 15:37:48 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=bst&pn=6>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 431, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 460, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 560, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 8969, in goto
    await self._impl_obj.goto(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_page.py", line 553, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_frame.py", line 145, in goto
    await self._channel.send("goto", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.goto: Connection closed while reading from the driver
2024-12-15 15:37:48 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=bst&pn=3>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 431, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 460, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 560, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 8969, in goto
    await self._impl_obj.goto(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_page.py", line 553, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_frame.py", line 145, in goto
    await self._channel.send("goto", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.goto: Connection closed while reading from the driver
2024-12-15 15:37:48 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=bst&pn=4>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 431, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 460, in _download_request_with_page
    response, download = await self._get_response_and_download(request, page, spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 560, in _get_response_and_download
    response = await page.goto(url=request.url, **page_goto_kwargs)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 8969, in goto
    await self._impl_obj.goto(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_page.py", line 553, in goto
    return await self._main_frame.goto(**locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_frame.py", line 145, in goto
    await self._channel.send("goto", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.goto: Connection closed while reading from the driver
2024-12-15 15:37:48 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=bst&pn=9>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 397, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 303, in _create_page
    page = await ctx_wrapper.context.new_page()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 12767, in new_page
    return mapping.from_impl(await self._impl_obj.new_page())
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_browser_context.py", line 326, in new_page
    return from_channel(await self._channel.send("newPage"))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserContext.new_page: Route.abort: Connection closed while reading from the driver
2024-12-15 15:37:48 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=bst&pn=10>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 397, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 303, in _create_page
    page = await ctx_wrapper.context.new_page()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 12767, in new_page
    return mapping.from_impl(await self._impl_obj.new_page())
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_browser_context.py", line 326, in new_page
    return from_channel(await self._channel.send("newPage"))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserContext.new_page: Connection closed while reading from the driver
2024-12-15 15:37:49 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=bst&pn=11>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 397, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 303, in _create_page
    page = await ctx_wrapper.context.new_page()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 12767, in new_page
    return mapping.from_impl(await self._impl_obj.new_page())
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_browser_context.py", line 326, in new_page
    return from_channel(await self._channel.send("newPage"))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserContext.new_page: Connection closed while reading from the driver
2024-12-15 15:37:49 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=bst&pn=12>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 397, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 303, in _create_page
    page = await ctx_wrapper.context.new_page()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 12767, in new_page
    return mapping.from_impl(await self._impl_obj.new_page())
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_browser_context.py", line 326, in new_page
    return from_channel(await self._channel.send("newPage"))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserContext.new_page: Connection closed while reading from the driver
2024-12-15 15:37:49 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=bst&pn=13>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 397, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 303, in _create_page
    page = await ctx_wrapper.context.new_page()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 12767, in new_page
    return mapping.from_impl(await self._impl_obj.new_page())
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_browser_context.py", line 326, in new_page
    return from_channel(await self._channel.send("newPage"))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserContext.new_page: Connection closed while reading from the driver
2024-12-15 15:37:50 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=bst&pn=14>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 397, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 303, in _create_page
    page = await ctx_wrapper.context.new_page()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 12767, in new_page
    return mapping.from_impl(await self._impl_obj.new_page())
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_browser_context.py", line 326, in new_page
    return from_channel(await self._channel.send("newPage"))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserContext.new_page: Connection closed while reading from the driver
2024-12-15 15:37:50 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=bst&pn=15>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 397, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 303, in _create_page
    page = await ctx_wrapper.context.new_page()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 12767, in new_page
    return mapping.from_impl(await self._impl_obj.new_page())
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_browser_context.py", line 326, in new_page
    return from_channel(await self._channel.send("newPage"))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserContext.new_page: Connection closed while reading from the driver
2024-12-15 15:37:50 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2024-12-15 15:38:31 [scrapy.utils.log] INFO: Scrapy 2.11.1 started (bot: sggwScraper)
2024-12-15 15:38:31 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 24.2.1 (OpenSSL 3.0.15 3 Sep 2024), cryptography 43.0.0, Platform Windows-10-10.0.19045-SP0
2024-12-15 15:38:31 [scrapy.addons] INFO: Enabled addons:
[]
2024-12-15 15:38:31 [scrapy.extensions.telnet] INFO: Telnet Password: 998b4f70a08ac5f8
2024-12-15 15:38:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.throttle.AutoThrottle']
2024-12-15 15:38:32 [scrapy.crawler] INFO: Overridden settings:
{'AUTOTHROTTLE_ENABLED': True,
 'AUTOTHROTTLE_MAX_DELAY': 30,
 'AUTOTHROTTLE_START_DELAY': 0.2,
 'AUTOTHROTTLE_TARGET_CONCURRENCY': 4,
 'BOT_NAME': 'sggwScraper',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_errors.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'sggwScraper.spiders',
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'RETRY_HTTP_CODES': [500],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['sggwScraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2024-12-15 15:38:32 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2024-12-15 15:38:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'sggwScraper.middlewares.SggwscraperDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2024-12-15 15:38:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2024-12-15 15:38:34 [scrapy.middleware] INFO: Enabled item pipelines:
['sggwScraper.pipelines.SggwscraperPipeline']
2024-12-15 15:38:34 [scrapy.core.engine] INFO: Spider opened
2024-12-15 15:38:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2024-12-15 15:38:34 [publications] INFO: Spider opened: publications
2024-12-15 15:38:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2024-12-15 15:38:34 [scrapy-playwright] INFO: Starting download handler
2024-12-15 15:38:34 [scrapy-playwright] INFO: Starting download handler
2024-12-15 15:38:35 [scrapy-playwright] INFO: Launching 3 startup context(s)
2024-12-15 15:38:35 [scrapy-playwright] INFO: Launching browser chromium
2024-12-15 15:38:35 [scrapy-playwright] INFO: Launching 3 startup context(s)
2024-12-15 15:38:35 [scrapy-playwright] INFO: Launching browser chromium
2024-12-15 15:38:35 [scrapy-playwright] INFO: Browser chromium launched
2024-12-15 15:38:35 [scrapy-playwright] INFO: Browser chromium launched
2024-12-15 15:38:35 [scrapy-playwright] INFO: Startup context(s) launched
2024-12-15 15:38:36 [scrapy-playwright] INFO: Startup context(s) launched
2024-12-15 15:39:34 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2024-12-15 15:40:34 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 12 pages/min), scraped 0 items (at 0 items/min)
2024-12-15 15:41:34 [scrapy.extensions.logstats] INFO: Crawled 342 pages (at 328 pages/min), scraped 324 items (at 324 items/min)
2024-12-15 15:42:34 [scrapy.extensions.logstats] INFO: Crawled 373 pages (at 31 pages/min), scraped 338 items (at 14 items/min)
2024-12-15 15:43:34 [scrapy.extensions.logstats] INFO: Crawled 700 pages (at 327 pages/min), scraped 664 items (at 326 items/min)
2024-12-15 15:43:57 [scrapy-playwright] INFO: Launching browser chromium
2024-12-15 15:43:57 [scrapy-playwright] INFO: Launching browser chromium
2024-12-15 15:43:58 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2024-12-15 15:43:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=bst&pn=2596> (referer: https://bw.sggw.edu.pl/index.seam)
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\defer.py", line 295, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\Desktop\PYTHON\WebScraping\SGGW_scraper\Scraper\sggwScraper\sggwScraper\spiders\publications.py", line 78, in parse_publications_links
    await page.close()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 9767, in close
    await self._impl_obj.close(runBeforeUnload=run_before_unload, reason=reason)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_page.py", line 809, in close
    raise e
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_page.py", line 804, in close
    await self._channel.send("close", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.close: Connection closed while reading from the driver
2024-12-15 15:43:58 [scrapy.core.engine] INFO: Closing spider (shutdown)
2024-12-15 15:43:58 [scrapy-playwright] INFO: Launching browser chromium
2024-12-15 15:43:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=bst&pn=2595>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 397, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 296, in _create_page
    ctx_wrapper = await self._create_browser_context(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 257, in _create_browser_context
    await self._maybe_launch_browser()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 205, in _maybe_launch_browser
    self.browser = await self.browser_type.launch(**self.config.launch_options)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 14398, in launch
    await self._impl_obj.launch(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_browser_type.py", line 95, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserType.launch: Connection closed while reading from the driver
2024-12-15 15:43:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=bst&pn=2594>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 397, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 296, in _create_page
    ctx_wrapper = await self._create_browser_context(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 257, in _create_browser_context
    await self._maybe_launch_browser()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 205, in _maybe_launch_browser
    self.browser = await self.browser_type.launch(**self.config.launch_options)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 14398, in launch
    await self._impl_obj.launch(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_browser_type.py", line 95, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserType.launch: Connection closed while reading from the driver
2024-12-15 15:43:58 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=bst&pn=2592>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 378, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 397, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 296, in _create_page
    ctx_wrapper = await self._create_browser_context(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 257, in _create_browser_context
    await self._maybe_launch_browser()
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\scrapy_playwright\handler.py", line 205, in _maybe_launch_browser
    self.browser = await self.browser_type.launch(**self.config.launch_options)
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\async_api\_generated.py", line 14398, in launch
    await self._impl_obj.launch(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_browser_type.py", line 95, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\WebScraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserType.launch: Connection closed while reading from the driver
2024-12-15 15:44:19 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
