2024-11-21 01:52:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://bw.sggw.edu.pl/info/report/WULSaf2701b9b5f5466982a0552a2e195bed?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BRodzina%2BSuidae%2B-%2Breprodukcja%2Ba%2Bzagro%25C5%25BCenia%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en> (referer: https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=hgu&pn=9)
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\defer.py", line 295, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\Desktop\-_-\scraping\Projekt_SGGW\Scraper\sggwScraper\sggwScraper\spiders\sggw.py", line 330, in parse_publication
    await page.close()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 9767, in close
    await self._impl_obj.close(runBeforeUnload=run_before_unload, reason=reason)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_page.py", line 809, in close
    raise e
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_page.py", line 804, in close
    await self._channel.send("close", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.close: Connection closed while reading from the driver
2024-11-21 01:52:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://bw.sggw.edu.pl/info/report/WULSc602067dc9e24458a48f6b1d2d4f9c27?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BWhat%2Bare%2Bthe%2Brequired%2Bsupporting%2Bmeasures%2Bto%2Bimprove%2Bthe%2Bbiosecurity%2Bof%2BSpanish%2Bfarms%253F%2B%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en> (referer: https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=hgu&pn=10)
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\defer.py", line 295, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\Desktop\-_-\scraping\Projekt_SGGW\Scraper\sggwScraper\sggwScraper\spiders\sggw.py", line 330, in parse_publication
    await page.close()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 9767, in close
    await self._impl_obj.close(runBeforeUnload=run_before_unload, reason=reason)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_page.py", line 809, in close
    raise e
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_page.py", line 804, in close
    await self._channel.send("close", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.close: Connection closed while reading from the driver
2024-11-21 01:52:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://bw.sggw.edu.pl/info/report/WULSc7992033fcea4e04b14eae2eee4872ef?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BMicrotine%2Brodents%2Bas%2Buniversal%2Bprey%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en> (referer: https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=hgu&pn=10)
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\defer.py", line 295, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\Desktop\-_-\scraping\Projekt_SGGW\Scraper\sggwScraper\sggwScraper\spiders\sggw.py", line 330, in parse_publication
    await page.close()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 9767, in close
    await self._impl_obj.close(runBeforeUnload=run_before_unload, reason=reason)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_page.py", line 809, in close
    raise e
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_page.py", line 804, in close
    await self._channel.send("close", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.close: Connection closed while reading from the driver
2024-11-21 01:52:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://bw.sggw.edu.pl/info/report/WULSc659ab634d63457b8c286766d6f4d3d9?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BEkspertyza%2Bmykologiczno-budowlana%2Bpomieszcze%25C5%2584%2Bmagazynowych%2B%2BTowarzystwa%2BUbezpiecze%25C5%2584%2Bna%2B%25C5%25BBycie%2BCommercial%2BUnion%2Bw%2BWarszawie%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en> (referer: https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=hgu&pn=10)
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\defer.py", line 295, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\Desktop\-_-\scraping\Projekt_SGGW\Scraper\sggwScraper\sggwScraper\spiders\sggw.py", line 330, in parse_publication
    await page.close()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 9767, in close
    await self._impl_obj.close(runBeforeUnload=run_before_unload, reason=reason)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_page.py", line 809, in close
    raise e
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_page.py", line 804, in close
    await self._channel.send("close", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.close: Connection closed while reading from the driver
2024-11-21 01:52:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://bw.sggw.edu.pl/info/report/WULSae32b93017af4faf899a2fd1dc92056c?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BEvaluation%2Bof%2Bthe%2Bbiosecurity%2Bcompliance%2Bin%2Bpoultry%2Bproduction%2Bin%2BPoland%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en> (referer: https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=hgu&pn=9)
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\defer.py", line 295, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\Desktop\-_-\scraping\Projekt_SGGW\Scraper\sggwScraper\sggwScraper\spiders\sggw.py", line 330, in parse_publication
    await page.close()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 9767, in close
    await self._impl_obj.close(runBeforeUnload=run_before_unload, reason=reason)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_page.py", line 809, in close
    raise e
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_page.py", line 804, in close
    await self._channel.send("close", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.close: Connection closed while reading from the driver
2024-11-21 01:52:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://bw.sggw.edu.pl/info/report/WULSae30b51a25e646d08c291a69af89abea?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BBadanie%2Bwarto%25C5%259Bci%2Bgrzybob%25C3%25B3jczej%2B%25C5%259Brodka%2Bochrony%2Bdrewna%2BIntox%2BS%2Bprzeciwko%2Bpodstawczakom%2Bmetod%25C4%2585%2Bagarowo-klockow%25C4%2585%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en> (referer: https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=hgu&pn=9)
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\defer.py", line 295, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\Desktop\-_-\scraping\Projekt_SGGW\Scraper\sggwScraper\sggwScraper\spiders\sggw.py", line 330, in parse_publication
    await page.close()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 9767, in close
    await self._impl_obj.close(runBeforeUnload=run_before_unload, reason=reason)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_page.py", line 809, in close
    raise e
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_page.py", line 804, in close
    await self._channel.send("close", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.close: Connection closed while reading from the driver
2024-11-21 01:52:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/report/WULSc2766f6ce63d4ad48376e197921bf0ac?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BSeroepidemiologia%2Bzaka%25C5%25BCe%25C5%2584%2Breowirusowych%2Bw%2Bstadach%2Bbrojler%25C3%25B3w%2Bkurzych%2B%25E2%2580%2593%2Bobserwacje%2Bd%25C5%2582ugoterminowe%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 379, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 398, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 304, in _create_page
    page = await ctx_wrapper.context.new_page()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 12767, in new_page
    return mapping.from_impl(await self._impl_obj.new_page())
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_browser_context.py", line 326, in new_page
    return from_channel(await self._channel.send("newPage"))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserContext.new_page: Connection closed while reading from the driver
2024-11-21 01:52:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/report/WULSa9cd4d1674b34f35a0fc27c365b0473e?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BBogactwo%2Bgatunkowe%2Bw%2BBia%25C5%2582owieskim%2BParku%2BNarodowym%253A%2Bdrobne%2Bssaki%2Bw%2Bsiedliskach%2Bturzycowych%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 379, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 398, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 304, in _create_page
    page = await ctx_wrapper.context.new_page()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 12767, in new_page
    return mapping.from_impl(await self._impl_obj.new_page())
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_browser_context.py", line 326, in new_page
    return from_channel(await self._channel.send("newPage"))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserContext.new_page: Connection closed while reading from the driver
2024-11-21 01:52:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/report/WULSa5e628bffa1f44f3a7313900165649a6?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BOcena%2Bbada%25C5%2584%2Bskuteczno%25C5%259Bci%2Bproduktu%2Bbiob%25C3%25B3jczego%2B%2522Drewnokolor%2BImpregnat%2Bdo%2BDrewna%2522%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 379, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 398, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 304, in _create_page
    page = await ctx_wrapper.context.new_page()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 12767, in new_page
    return mapping.from_impl(await self._impl_obj.new_page())
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_browser_context.py", line 326, in new_page
    return from_channel(await self._channel.send("newPage"))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserContext.new_page: Connection closed while reading from the driver
2024-11-21 01:52:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/report/WULSab3aaaadb00b42d2bb501b9bfb66a4f3?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BBiosecurity%2Bgaps%2Bin%2Bseven%2BEuropean%2Bpoultry%2B%2528breeder%2Band%2Blayer%2Bfarms%2529%2B%25E2%2580%2593%2Ba%2Bfarmers%2Bperspective%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 379, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 398, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 304, in _create_page
    page = await ctx_wrapper.context.new_page()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 12767, in new_page
    return mapping.from_impl(await self._impl_obj.new_page())
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_browser_context.py", line 326, in new_page
    return from_channel(await self._channel.send("newPage"))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserContext.new_page: Connection closed while reading from the driver
2024-11-21 01:52:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/report/WULSc1a58bb46ff04c699945f13278c87ee6?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BWhat%2Bis%2Bthe%2Bfuture%2Bof%2Bthe%2BEuropean%2Bbison%2Bpopulation%253F%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 379, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 398, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 304, in _create_page
    page = await ctx_wrapper.context.new_page()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 12767, in new_page
    return mapping.from_impl(await self._impl_obj.new_page())
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_browser_context.py", line 326, in new_page
    return from_channel(await self._channel.send("newPage"))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserContext.new_page: Connection closed while reading from the driver
2024-11-21 01:52:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/report/WULSab4f7eac11bf4ffdb9b3b056d218ba59?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BOcena%2Bbada%25C5%2584%2Bskuteczno%25C5%259Bci%2Bproduktu%2Bbiob%25C3%25B3jczego%2B%2522Impregnat%2Bdo%2Bdrewna%2Bbudowlano-konstrukcyjnego%2BDen-Braven%2BBosman%2B-%2Bkoncentrat%2522%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 379, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 398, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 304, in _create_page
    page = await ctx_wrapper.context.new_page()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 12767, in new_page
    return mapping.from_impl(await self._impl_obj.new_page())
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_browser_context.py", line 326, in new_page
    return from_channel(await self._channel.send("newPage"))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserContext.new_page: Connection closed while reading from the driver
2024-11-21 01:52:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/report/WULSac02feffd77244b0b358ddc7c52be496?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BBadanie%2Bpor%25C3%25B3wnawcze%2Bprodukt%25C3%25B3w%2Bdo%2Bochrony%2Bi%2Bpowierzchni%2Bprzed%2Bkurzem%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 379, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 398, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 304, in _create_page
    page = await ctx_wrapper.context.new_page()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 12767, in new_page
    return mapping.from_impl(await self._impl_obj.new_page())
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_browser_context.py", line 326, in new_page
    return from_channel(await self._channel.send("newPage"))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserContext.new_page: Connection closed while reading from the driver
2024-11-21 01:52:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/report/WULSc2daedf92bda437a897540b471385375?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BWybi%25C3%25B3rczo%25C5%259B%25C4%2587%2B%25C5%259Brodowiskowa%2Bryj%25C3%25B3wek%2Bna%2Bturzycowisku%253A%2Bwynik%2Bkonkurencji%2Bczy%2Bkoegzystencji%253F%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 379, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 398, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 304, in _create_page
    page = await ctx_wrapper.context.new_page()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 12767, in new_page
    return mapping.from_impl(await self._impl_obj.new_page())
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_browser_context.py", line 326, in new_page
    return from_channel(await self._channel.send("newPage"))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserContext.new_page: Connection closed while reading from the driver
2024-11-21 01:52:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/report/WULS96c12c890b5544409cf65b4bcf7d3e1e?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BBadania%2Bpo%25C5%25BCywkowe%2Bw%25C5%2582a%25C5%259Bciwo%25C5%259Bci%2Bbiocydowych%2Bpreparatu%2BMycobiol%2BC%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 379, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 398, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 297, in _create_page
    ctx_wrapper = await self._create_browser_context(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 258, in _create_browser_context
    await self._maybe_launch_browser()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 206, in _maybe_launch_browser
    self.browser = await self.browser_type.launch(**self.config.launch_options)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 14398, in launch
    await self._impl_obj.launch(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_browser_type.py", line 95, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserType.launch: Connection closed while reading from the driver
2024-11-21 01:52:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/report/WULSa2fcc40c1c9f4fdebf7ca32bef701c95?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BOcena%2Bbada%25C5%2584%2Bskuteczno%25C5%259Bci%2Bproduktu%2Bbiob%25C3%25B3jczego%2B%2522Protector%2BR%2BImpregnat%2Bkoloryzuj%25C4%2585cy%2Bdo%2Bdrewna%2522%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 379, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 398, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 297, in _create_page
    ctx_wrapper = await self._create_browser_context(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 258, in _create_browser_context
    await self._maybe_launch_browser()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 206, in _maybe_launch_browser
    self.browser = await self.browser_type.launch(**self.config.launch_options)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 14398, in launch
    await self._impl_obj.launch(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_browser_type.py", line 95, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserType.launch: Connection closed while reading from the driver
2024-11-21 01:52:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://bw.sggw.edu.pl/info/report/WULSc428e3f08c594dbdba2e8c3faed1a5c2?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BOcena%2Bbada%25C5%2584%2Bskuteczno%25C5%259Bci%2Bproduktu%2Bbiob%25C3%25B3jczego%2B%2522Korasit%2BKS%2B2%2522%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en> (referer: https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=hgu&pn=10)
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\defer.py", line 295, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\Desktop\-_-\scraping\Projekt_SGGW\Scraper\sggwScraper\sggwScraper\spiders\sggw.py", line 330, in parse_publication
    await page.close()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 9767, in close
    await self._impl_obj.close(runBeforeUnload=run_before_unload, reason=reason)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_page.py", line 809, in close
    raise e
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_page.py", line 804, in close
    await self._channel.send("close", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.close: Connection closed while reading from the driver
2024-11-21 01:52:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://bw.sggw.edu.pl/info/report/WULSad9ba4fb4ba54309a7d1456a99a8c2c0?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BQualitative%2Bassessment%2Bof%2Bbiosecurity%2Bcompliance%2Bin%2Bmeat%2Bpoultry%2B%2528commercial%2Bbroiler%2Band%2Bturkey%2Bproduction%2529%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en> (referer: https://bw.sggw.edu.pl/globalResultList.seam?r=publication&tab=PUBLICATION&lang=en&p=hgu&pn=9)
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\defer.py", line 295, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 374, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\python.py", line 355, in _async_chain
    async for o in as_async_generator(it):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\utils\asyncgen.py", line 14, in as_async_generator
    async for r in it:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 31, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 355, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 30, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 35, in process_spider_output_async
    async for r in result or ():
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\spidermw.py", line 118, in process_async
    async for r in iterable:
  File "C:\Users\Kamil\Desktop\-_-\scraping\Projekt_SGGW\Scraper\sggwScraper\sggwScraper\spiders\sggw.py", line 330, in parse_publication
    await page.close()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 9767, in close
    await self._impl_obj.close(runBeforeUnload=run_before_unload, reason=reason)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_page.py", line 809, in close
    raise e
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_page.py", line 804, in close
    await self._channel.send("close", locals_to_params(locals()))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: Page.close: Connection closed while reading from the driver
2024-11-21 01:52:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/report/WULS959499d6457748a7896794d673c8dbae?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BFactors%2Bimpacting%2Barea%2Busage%2Bby%2Bbank%2Bvoles%2B%25E2%2580%2593%2Bcoarse%2Bwoody%2Bdebris%2Band%2Bforest%2Bundergrowthphytocenosis%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 379, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 398, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 297, in _create_page
    ctx_wrapper = await self._create_browser_context(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 258, in _create_browser_context
    await self._maybe_launch_browser()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 206, in _maybe_launch_browser
    self.browser = await self.browser_type.launch(**self.config.launch_options)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 14398, in launch
    await self._impl_obj.launch(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_browser_type.py", line 95, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserType.launch: Connection closed while reading from the driver
2024-11-21 01:52:11 [scrapy.core.scraper] ERROR: Error downloading <GET https://bw.sggw.edu.pl/info/report/WULSa2d78f500fe649c6adfb505ee89e7b2c?r=publication&ps=20&tab=&title=Report%2B%25E2%2580%2593%2BOcena%2Bbada%25C5%2584%2Bskuteczno%25C5%259Bci%2Bproduktu%2Bbiob%25C3%25B3jczego%2B%2522Decor%2Bbezbarwny%2522%2B%25E2%2580%2593%2BWarsaw%2BUniversity%2Bof%2BLife%2BSciences%2B-%2BSGGW&lang=en>
Traceback (most recent call last):
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1996, in _inlineCallbacks
    result = context.run(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\python\failure.py", line 519, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy\core\downloader\middleware.py", line 54, in process_request
    return (yield download_func(request=request, spider=spider))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\twisted\internet\defer.py", line 1248, in adapt
    extracted: _SelfResultT | Failure = result.result()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\_utils.py", line 120, in _handle_coro
    future.set_result(await coro)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 379, in _download_request
    return await self._download_request_with_retry(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 398, in _download_request_with_retry
    page = await self._create_page(request=request, spider=spider)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 297, in _create_page
    ctx_wrapper = await self._create_browser_context(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 258, in _create_browser_context
    await self._maybe_launch_browser()
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\scrapy_playwright\handler.py", line 206, in _maybe_launch_browser
    self.browser = await self.browser_type.launch(**self.config.launch_options)
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\async_api\_generated.py", line 14398, in launch
    await self._impl_obj.launch(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_browser_type.py", line 95, in launch
    Browser, from_channel(await self._channel.send("launch", params))
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 59, in send
    return await self._connection.wrap_api_call(
  File "C:\Users\Kamil\anaconda3\envs\Scraping\Lib\site-packages\playwright\_impl\_connection.py", line 520, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
Exception: BrowserType.launch: Connection closed while reading from the driver
